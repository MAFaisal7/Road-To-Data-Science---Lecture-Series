---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
# read in data
library(readr)

train_orig <- read_csv("train.csv")

test_orig <- read_csv("test.csv")

# save the training labels
train_orig_labels <- train_orig[, 1]
train_orig_labels <- as.factor(train_orig_labels$label)
summary(train_orig_labels)

library(randomForest)
numTrees <- 25

# Train on entire training dataset and predict on the test
startTime <- proc.time()
rf <- randomForest(train_orig[-1], train_orig_labels, xtest=test_orig, 
                   ntree=numTrees)
proc.time() - startTime

rf

# output predictions for submission
predictions <- data.frame(ImageId=1:nrow(test_orig), 
                          Label=levels(train_orig_labels)[rf$test$predicted])
head(predictions)

library(nnet)

# split the training data into train and test to do local evaluation
set.seed(123)
rows <- sample(1:nrow(train_orig), as.integer(0.7*nrow(train_orig)))

# Get train and test labels
train_labels <- train_orig[rows, 1]
test_labels <- train_orig[-rows, 1]

# convert the labels to factors
train_labels <- as.factor(train_labels$label)
# custom normalization function
normalize <- function(x) { 
  return(x / 255)
}

# create the train and test datasets and apply normalization
train_norm <- as.data.frame(lapply(train_orig[rows, -1], normalize))
test_norm <- as.data.frame(lapply(train_orig[-rows,-1], normalize))

# check a random pixel to see if the normalization worked
summary(train_orig$pixel350)

summary(train_norm$pixel350)

summary(test_norm$pixel350)

# create the class indicator matrix
train_labels_matrix = class.ind(train_labels)
head(train_labels)

head(train_labels_matrix)

# train model
set.seed(123)
startTime <- proc.time()
nn = nnet(train_norm, train_labels_matrix, 
            size = 1, 
            softmax = TRUE 
            )

proc.time() - startTime

nn

# get predictions
pred = predict(nn, test_norm, type="class")
cbind(head(pred), head(test_labels))

# evaluate the model
accuracy <- mean(pred == test_labels)
print(paste('Accuracy:', accuracy))

library(caret)

# Enable parallel processing
library(doParallel)

# subtract 1 from detectCores() to reduce the number of cores used
no_cores <- detectCores() 
cl <- makeCluster(no_cores)
cl
registerDoParallel(cl)

# Set up training parameters
TrainingParameters <- trainControl(method = "cv", number = 3)

grid_nn <- expand.grid(.size = c(1, 3, 5, 10),
                       .decay = 0)
grid_nn

# use all of the given training data
train_norm <- as.data.frame(lapply(train_orig[, -1], normalize))

startTime <- proc.time()
set.seed(123)
nn2 <- train(train_norm, train_orig_labels,
             trControl= TrainingParameters,
             method = "nnet",
             tuneGrid = grid_nn,
             MaxNWts = 20000
             )
proc.time() - startTime

nn2

# normalize test data and predict on it
test_norm <- as.data.frame(lapply(test_orig, normalize))
NNPredictions <-predict(nn2, test_norm)
# output predictions for submission
predictions <- data.frame(ImageId=1:nrow(test_orig), 
                          Label=levels(train_orig_labels)[NNPredictions])
head(predictions)

# stop the cluster for parallel processing
stopCluster(cl)


